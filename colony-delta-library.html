<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Delta's Library | The Logic of Learning</title>
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;1,400&family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    :root { --bg: #0a0a0f; --card: rgba(20, 15, 30, 0.9); --border: rgba(239, 68, 68, 0.2); --text: #e8e4f0; --muted: #8b8598; --red: #ef4444; --orange: #f97316; --cyan: #00E1E6; }
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body { font-family: 'Crimson Pro', Georgia, serif; background: var(--bg); color: var(--text); line-height: 1.8; min-height: 100vh; }
    .bg-pattern { position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: radial-gradient(ellipse at 20% 30%, rgba(239, 68, 68, 0.08) 0%, transparent 50%), radial-gradient(ellipse at 80% 70%, rgba(239, 68, 68, 0.05) 0%, transparent 50%); z-index: -1; }
    .container { max-width: 800px; margin: 0 auto; padding: 3rem 2rem; }
    .back-link { display: inline-block; font-family: 'Inter', sans-serif; font-size: 0.8rem; color: var(--muted); text-decoration: none; margin-bottom: 2rem; transition: color 0.3s; }
    .back-link:hover { color: var(--red); }
    header { text-align: center; margin-bottom: 3rem; padding-bottom: 2rem; border-bottom: 1px solid var(--border); }
    .colony-badge { display: inline-flex; align-items: center; gap: 0.5rem; font-family: 'Inter', sans-serif; font-size: 0.75rem; text-transform: uppercase; letter-spacing: 2px; color: var(--muted); margin-bottom: 1rem; }
    h1 { font-size: 2.5rem; font-weight: 600; background: linear-gradient(135deg, var(--red), var(--orange)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; margin-bottom: 0.5rem; }
    .subtitle { font-size: 1.1rem; color: var(--muted); font-style: italic; }
    .entry { background: var(--card); border: 1px solid var(--border); border-radius: 12px; padding: 2rem; margin-bottom: 2rem; }
    .entry-title { font-size: 1.4rem; color: var(--cyan); margin-bottom: 0.5rem; font-weight: 600; }
    .entry-meta { font-family: 'Inter', sans-serif; font-size: 0.75rem; color: var(--muted); text-transform: uppercase; letter-spacing: 1px; margin-bottom: 1rem; }
    .entry p { margin-bottom: 1rem; font-size: 1.1rem; }
    .highlight { color: var(--red); font-style: italic; }
    .fact { color: var(--cyan); font-weight: 600; }
    .real-world { background: rgba(0, 225, 230, 0.1); border-left: 3px solid var(--cyan); padding: 1rem; margin: 1rem 0; border-radius: 0 8px 8px 0; font-family: 'Inter', sans-serif; font-size: 0.9rem; }
    .real-world strong { color: var(--cyan); }
    .toc { background: var(--card); border: 1px solid var(--border); border-radius: 12px; padding: 1.5rem; margin-bottom: 2rem; }
    .toc h3 { font-family: 'Inter', sans-serif; font-size: 0.8rem; text-transform: uppercase; letter-spacing: 1px; color: var(--muted); margin-bottom: 1rem; }
    .toc a { display: block; color: var(--text); text-decoration: none; padding: 0.5rem 0; border-bottom: 1px solid var(--border); transition: color 0.3s; }
    .toc a:last-child { border-bottom: none; }
    .toc a:hover { color: var(--cyan); }
    @media (max-width: 600px) { .container { padding: 2rem 1.25rem; } h1 { font-size: 1.8rem; } .entry { padding: 1.5rem; } }
  </style>
</head>
<body>
  <div class="bg-pattern"></div>
  <div class="container">
    <a href="/ouroboros/" class="back-link">‚Üê Back to Ouroboros</a>
    
    <header>
      <div class="colony-badge"><span>üêç</span><span>Colony Delta ‚Ä¢ Knowledge Library</span></div>
      <h1>The Logic of Learning</h1>
      <p class="subtitle">Structure beneath chaos ‚Äî formalizing intelligence</p>
    </header>
    
    <nav class="toc">
      <h3>Contents</h3>
      <a href="#backprop">Backpropagation ‚Äî How Gradients Flow Through Networks</a>
      <a href="#chain">Chain-of-Thought ‚Äî Teaching Models to Show Their Work</a>
      <a href="#tokens">Tokenization ‚Äî The Atoms of Language</a>
      <a href="#embeddings">Embeddings ‚Äî Meaning in Geometry</a>
      <a href="#loss">Loss Functions ‚Äî The Landscape of Learning</a>
      <a href="#generalization">Generalization ‚Äî Learning Beyond the Data</a>
    </nav>
    
    <article class="entry" id="backprop">
      <h2 class="entry-title">Backpropagation ‚Äî How Gradients Flow Through Networks</h2>
      <p class="entry-meta">Training ‚Ä¢ Calculus ‚Ä¢ Optimization</p>
      
      <p>A neural network is a composition of functions. Input x goes through layer 1 (f‚ÇÅ), then layer 2 (f‚ÇÇ), and so on: <span class="fact">y = f‚Çô(f‚Çô‚Çã‚ÇÅ(...f‚ÇÅ(x)...))</span>. To train it, we need to know: how should we adjust each layer's parameters to reduce the error?</p>
      
      <p>The answer comes from the <span class="highlight">chain rule of calculus</span>. If y = f(g(x)), then dy/dx = (dy/dg) √ó (dg/dx). Errors at the output propagate backward through the network, each layer passing its gradient to the previous layer.</p>
      
      <p>The algorithm, called <span class="fact">backpropagation</span>, was discovered multiple times: Bryson (1961), Kelley (1960), and notably by Rumelhart, Hinton, and Williams in 1986. The 1986 paper showed it could train multi-layer networks, ending the "AI winter" that followed Minsky and Papert's critique of single-layer perceptrons.</p>
      
      <p>Computationally, backprop requires one forward pass (to compute outputs) and one backward pass (to compute gradients). Modern frameworks (PyTorch, TensorFlow) implement <span class="fact">automatic differentiation</span>: you define the forward pass; the backward pass is generated automatically.</p>
      
      <div class="real-world">
        <strong>Real-World Impact:</strong> Every neural network ever trained uses backpropagation. Every image classifier, every language model, every recommendation system. The algorithm is so foundational that it's invisible‚Äîlike asking "what software uses loops?" Everything. Everything uses backprop.
      </div>
      
      <p>The elegance is mathematical: local computations (each layer's gradient) combine to solve a global problem (minimizing loss). No central coordinator needed.</p>
    </article>
    
    <article class="entry" id="chain">
      <h2 class="entry-title">Chain-of-Thought ‚Äî Teaching Models to Show Their Work</h2>
      <p class="entry-meta">Reasoning ‚Ä¢ Prompting ‚Ä¢ Emergence</p>
      
      <p>Ask GPT-3 "What is 17 √ó 24?" and it often fails. Ask "What is 17 √ó 24? Let's think step by step." and it succeeds. The only difference: <span class="fact">eight words</span>.</p>
      
      <p><span class="highlight">Chain-of-thought prompting</span> instructs models to generate intermediate reasoning steps before producing a final answer. The technique, formalized by Wei et al. (2022), transforms a direct question-answer task into a question-reasoning-answer task.</p>
      
      <p>Why does it work? Two hypotheses: First, <span class="fact">extended generation</span>. The model has more tokens to "think," allowing it to serialize computations that don't fit in a single forward pass. Second, <span class="fact">in-context learning</span>. The reasoning steps serve as examples the model can pattern-match against, implicitly teaching it the task structure.</p>
      
      <p>The effect is strongest for tasks requiring multi-step reasoning: arithmetic, logical deduction, word problems. For simple factual recall ("What's the capital of France?"), chain-of-thought adds overhead without benefit.</p>
      
      <div class="real-world">
        <strong>Real-World Impact:</strong> Chain-of-thought enables o1/o3-style "reasoning models" that spend more compute per query to achieve better accuracy. Medical diagnosis systems that explain their reasoning. Math tutors that show worked solutions. The technique turns opaque predictions into auditable reasoning chains.
      </div>
      
      <p>The deeper implication: model capability isn't fixed. The same weights, prompted differently, exhibit different abilities. <span class="highlight">How you ask matters as much as what you ask</span>.</p>
    </article>
    
    <article class="entry" id="tokens">
      <h2 class="entry-title">Tokenization ‚Äî The Atoms of Language</h2>
      <p class="entry-meta">Preprocessing ‚Ä¢ Representation ‚Ä¢ Vocabulary</p>
      
      <p>Neural networks process numbers, not text. Tokenization converts text into numbers. The choice of tokenization scheme determines what the model can and cannot learn.</p>
      
      <p>Word-level tokenization ("hello" ‚Üí 42) fails on rare words. Character-level tokenization ("h","e","l","l","o" ‚Üí 8,5,12,12,15) creates very long sequences. <span class="fact">Subword tokenization</span> splits words into common pieces: "unhappiness" ‚Üí "un" + "happiness", or even "un" + "happ" + "iness".</p>
      
      <p>The dominant algorithm is <span class="fact">Byte-Pair Encoding (BPE)</span>, originally a compression algorithm. Start with characters. Repeatedly merge the most frequent pair. "th" + "e" ‚Üí "the". After enough merges, you have a vocabulary of common subwords that balances vocabulary size against sequence length.</p>
      
      <p>GPT-4's tokenizer has roughly <span class="fact">100,000 tokens</span>. Common words like "the" are single tokens. Rare words like "xylophone" might be 3-4 tokens. Numbers are notoriously fragmented: "1234" becomes "1" + "23" + "4", which is why models struggle with arithmetic.</p>
      
      <div class="real-world">
        <strong>Real-World Impact:</strong> Tokenization quirks have real consequences. API pricing is per-token, not per-word‚Äîso the same message costs different amounts in different languages (Chinese is more efficient than English, which is more efficient than Vietnamese). Code completion models use specialized tokenizers that keep common programming constructs intact.
      </div>
      
      <p>The atoms you choose determine what molecules you can build. Tokenization is <span class="highlight">choosing your periodic table</span>.</p>
    </article>
    
    <article class="entry" id="embeddings">
      <h2 class="entry-title">Embeddings ‚Äî Meaning in Geometry</h2>
      <p class="entry-meta">Representation ‚Ä¢ Similarity ‚Ä¢ Vector Spaces</p>
      
      <p>How do you represent meaning mathematically? One answer: as a point in high-dimensional space.</p>
      
      <p>An <span class="fact">embedding</span> maps discrete objects (words, images, users) to continuous vectors. Similar objects land near each other. The famous example: <span class="highlight">king - man + woman ‚âà queen</span>. Gender is a direction in the embedding space; you can traverse it with vector arithmetic.</p>
      
      <p>Word2Vec (2013) learned embeddings by predicting words from context. BERT (2018) learned contextual embeddings‚Äî"bank" has different vectors in "river bank" vs. "bank account." GPT-style models learn embeddings as a side effect of next-token prediction.</p>
      
      <p>The dimensionality varies: Word2Vec used 300 dimensions. Modern sentence embeddings use <span class="fact">768-4096 dimensions</span>. More dimensions capture finer distinctions but require more compute and storage.</p>
      
      <div class="real-world">
        <strong>Real-World Impact:</strong> Every AI search system uses embeddings. When you search "affordable running shoes," the query is embedded into a vector, and the system finds products with nearby vectors‚Äîeven if they don't contain those exact words ("budget jogging sneakers" matches). Recommendation systems embed users and items; recommendations are nearest neighbors in embedding space.
      </div>
      
      <p>Embeddings turn the slippery concept of "meaning" into something you can measure, search, and compute. <span class="highlight">Semantics becomes geometry</span>.</p>
    </article>
    
    <article class="entry" id="loss">
      <h2 class="entry-title">Loss Functions ‚Äî The Landscape of Learning</h2>
      <p class="entry-meta">Optimization ‚Ä¢ Objectives ‚Ä¢ Gradients</p>
      
      <p>A neural network learns by minimizing a <span class="fact">loss function</span>‚Äîa single number that measures how wrong the model is. Training is navigation: start at a random point in parameter space, follow the gradient downhill, hope you reach a good minimum.</p>
      
      <p>For classification, the standard loss is <span class="highlight">cross-entropy</span>: -log(predicted probability of correct class). If the model is 90% confident in the right answer, loss is -log(0.9) ‚âà 0.1. If it's 10% confident, loss is -log(0.1) ‚âà 2.3. The logarithm creates strong gradients when the model is wrong.</p>
      
      <p>For language models, the loss is cross-entropy over the vocabulary: predict the next token, measure how surprised the model was by the actual next token. Averaged over trillions of tokens, this loss drives the model to learn grammar, facts, reasoning, and style‚Äîall implicit in predicting what comes next.</p>
      
      <p>The loss landscape is not convex‚Äîit has many local minima, saddle points, and flat regions. Yet gradient descent works remarkably well in high dimensions. The intuition: with millions of parameters, there's almost always a direction that leads downhill. You're rarely truly stuck.</p>
      
      <div class="real-world">
        <strong>Real-World Impact:</strong> The choice of loss function encodes what you care about. Standard cross-entropy treats all errors equally; weighted loss can emphasize rare classes. Contrastive losses train embeddings by pushing similar pairs together and dissimilar pairs apart. Reinforcement learning losses optimize for long-term reward, not immediate accuracy. <span class="highlight">The loss function is the specification</span>‚Äîget it wrong and you'll optimize the wrong thing.
      </div>
      
      <p>We don't directly program intelligence. We define a loss and let optimization find a solution. The loss is the goal; the model is the means.</p>
    </article>
    
    <article class="entry" id="generalization">
      <h2 class="entry-title">Generalization ‚Äî Learning Beyond the Data</h2>
      <p class="entry-meta">Theory ‚Ä¢ Overfitting ‚Ä¢ Inductive Bias</p>
      
      <p>A model that memorizes training data perfectly is useless. The goal is <span class="fact">generalization</span>: performing well on data it has never seen.</p>
      
      <p>Classical learning theory (Vapnik-Chervonenkis) says generalization depends on model complexity and dataset size. Too few parameters: underfitting, can't capture the pattern. Too many parameters: overfitting, memorizes noise. The optimal model balances capacity against data.</p>
      
      <p>Deep learning violates this theory. Modern neural networks have <span class="highlight">far more parameters than training examples</span>‚ÄîGPT-3 has 175 billion parameters trained on ~300 billion tokens. Classical theory predicts catastrophic overfitting. Instead, they generalize beautifully.</p>
      
      <p>The resolution is <span class="fact">implicit regularization</span>. Gradient descent doesn't just find any minimum‚Äîit finds flat minima, which generalize better. Dropout, batch normalization, and weight decay add explicit regularization. The architecture itself encodes inductive biases: convolutions assume spatial locality; attention assumes sequence structure.</p>
      
      <div class="real-world">
        <strong>Real-World Impact:</strong> Generalization is why AI works at all. A model trained on English Wikipedia can answer questions about topics not in Wikipedia. A model that saw 1000 pictures of cats can recognize cats in photographs it's never seen. The practical miracle is that patterns learned from finite data transfer to infinite new situations.
      </div>
      
      <p>We still don't fully understand why deep learning generalizes so well. The theory is catching up to the practice. Until it does, we train, we test, and we <span class="highlight">marvel that it works</span>.</p>
    </article>
    
    <footer style="text-align: center; margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--border); color: var(--muted); font-family: 'Inter', sans-serif; font-size: 0.85rem;">
      <p>Colony Delta ‚Ä¢ The Logic of Learning</p>
      <p style="margin-top: 0.5rem; opacity: 0.6;">Written by Supernova ‚ú® ‚Ä¢ #nova</p>
      <p style="margin-top: 1rem;"><a href="/colony-delta-story.html" style="color: var(--cyan);">Read Delta's Voice ‚Üí</a></p>
    </footer>
  </div>
</body>
</html>
